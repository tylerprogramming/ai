{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U 'crewai[tools]'\n",
    "%pip install -U crewai\n",
    "%pip install -U requests\n",
    "%pip install -U python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Crew, Agent, Task\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from crewai_tools import ScrapeWebsiteTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or \"<your_openai_key>\"\n",
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\") or \"<your_serper_key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import BaseTool\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class CustomSerperDevTool(BaseTool):\n",
    "    name: str = \"Custom Serper Dev Tool\"\n",
    "    description: str = \"Search the internet for news.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Search the internet for news.\n",
    "        \"\"\"\n",
    "\n",
    "        url = \"https://google.serper.dev/news\"\n",
    "\n",
    "        payload = json.dumps({\n",
    "            \"q\": query,\n",
    "            \"num\": 20,\n",
    "            \"autocorrect\": False,\n",
    "            \"tbs\": \"qdr:d\"\n",
    "        })\n",
    "\n",
    "        headers = {\n",
    "            'X-API-KEY': SERPER_API_KEY,\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "        # Parse the JSON response\n",
    "        response_data = response.json()\n",
    "\n",
    "        # Extract only the 'news' property\n",
    "        news_data = response_data.get('news', [])\n",
    "\n",
    "        # Convert the news data back to a JSON string\n",
    "        return json.dumps(news_data, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_serper_dev_tool = CustomSerperDevTool()\n",
    "scrape_website_tool = ScrapeWebsiteTool()\n",
    "\n",
    "class Agents:\n",
    "    def researcher(self, topic):\n",
    "        return Agent(\n",
    "            role=f\"{topic} Senior News Researcher\",\n",
    "            goal=f\"Uncover latest news in {topic}\",\n",
    "            tools=[custom_serper_dev_tool, scrape_website_tool],\n",
    "            backstory=f\"You're a seasoned researcher with a knack for uncovering the latest developments in {topic}. Known for your ability to find the most relevant information and present it in a clear and concise manner.\",\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    def reporting_analyst(self, topic):\n",
    "        return Agent(\n",
    "            role=f\"{topic} News Reporting Analyst\",\n",
    "            goal=f\"Create detailed reports based on {topic} news analysis and research findings\",\n",
    "            tools=[],\n",
    "            backstory=f\"You're a meticulous analyst with a keen eye for detail. You're known for your ability to turn complex data into clear and concise reports, making it easy for others to understand and act on the information you provide.\",\n",
    "            verbose=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "class Tasks:\n",
    "    def research_task(self, agent, topic):\n",
    "        return Task(\n",
    "            description=dedent(\n",
    "                f\"\"\"\n",
    "                    Search news about {topic}\n",
    "                    \"\"\"\n",
    "            ),\n",
    "            expected_output=dedent(\n",
    "                f\"\"\"A list of news articles about {topic} with the title, url and snippet\"\"\"\n",
    "            ),\n",
    "            agent=agent,\n",
    "        )\n",
    "\n",
    "    def reporting_task(self, agent):\n",
    "        return Task(\n",
    "            description=dedent(\n",
    "                f\"\"\"\n",
    "                    Review the context you got.\n",
    "                    Make sure the report is detailed and contains any and all relevant information.\n",
    "                \"\"\"\n",
    "            ),\n",
    "            expected_output=dedent(\n",
    "                \"\"\"A fully fledge reporting of the news articles.\n",
    "                Formatted as markdown without '```'\"\"\"\n",
    "            ),\n",
    "            agent=agent,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tasks = Tasks()\n",
    "agents = Agents()\n",
    "\n",
    "topic = \"ai agents\"\n",
    "\n",
    "# Create Agents\n",
    "researcher = agents.researcher(topic=topic)\n",
    "reporting_analyst = agents.reporting_analyst(topic=topic)\n",
    "\n",
    "# Define Tasks for each agent\n",
    "research_task = tasks.research_task(agent=researcher, topic=topic, date=datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "reporting_task = tasks.reporting_task(agent=reporting_analyst)\n",
    "\n",
    "# Instantiate the crew with a sequential process\n",
    "crew = Crew(\n",
    "    agents=[researcher, reporting_analyst],\n",
    "    tasks=[\n",
    "        research_task,\n",
    "        reporting_task,\n",
    "    ],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from crewai.flow.flow import Flow, listen, start, and_\n",
    "from litellm import completion\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class News(BaseModel):\n",
    "    news: str = \"\"\n",
    "\n",
    "class NewsFlow(Flow[News]):\n",
    "    model = \"gpt-4o-mini\"\n",
    "    model_4o = \"gpt-4o\"\n",
    "\n",
    "    @start()\n",
    "    def generate_news_topic(self):\n",
    "        print(\"Starting flow\")\n",
    "\n",
    "        response = completion(\n",
    "            model=self.model_4o,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"\"\"Return a topic within the ai world that is trending.  \n",
    "                    This should be 1 - 4 words.\"\"\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        news_topic = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        print(f\"News Topic: {news_topic}\")\n",
    "\n",
    "        return news_topic\n",
    "\n",
    "    @listen(generate_news_topic)\n",
    "    def generate_news(self, news_topic):\n",
    "        print(\"Generating news with Crew\")\n",
    "\n",
    "        inputs = {\n",
    "            'topic': news_topic\n",
    "        }\n",
    "\n",
    "        result = crew.kickoff(inputs=inputs)\n",
    "\n",
    "        # get raw output then save to state\n",
    "        output = result.raw\n",
    "        self.state.news = output\n",
    "\n",
    "        return output\n",
    "\n",
    "    @listen(generate_news)\n",
    "    def save_news(self, news):\n",
    "        print(\"Saving news\")\n",
    "        \n",
    "        # Create the news directory if it doesn't exist\n",
    "        news_dir = os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"news\")\n",
    "        os.makedirs(news_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the news in the news directory\n",
    "        news_file_path = os.path.join(news_dir, \"news.md\")\n",
    "        with open(news_file_path, \"w\") as f:\n",
    "            f.write(news)\n",
    "        \n",
    "        print(f\"News saved to: {news_file_path}\")\n",
    "\n",
    "    @listen(generate_news)\n",
    "    def generate_best_news(self, input):\n",
    "        print(\"Generating best news\")\n",
    "        \n",
    "        response = completion(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Choose the most important news from the following and return it: {input}\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        important_news = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return important_news\n",
    "    \n",
    "    @listen(and_(generate_news_topic, generate_news, save_news, generate_best_news))\n",
    "    def logger(self, result):\n",
    "        print(f\"Logger: {result}\")\n",
    "        print(\"*\" * 100)\n",
    "        print(\"News Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    flow = NewsFlow()\n",
    "    flow.plot(\"my_flow_plot\")\n",
    "\n",
    "    await flow.kickoff()\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
